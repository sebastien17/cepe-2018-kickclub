---
title: "Bagging Analysis"
output: html_notebook
---

Bagging Analysis
----------------

Library Loading
```{r include=FALSE}
library(caret)
library(MLmetrics)
library(parallel)
library(doParallel)
library(reshape2)
library(ggplot2)
library(e1071)
library(rlist)
```

Functions Declaration
```{r}
cm.plot <- function(table_cm){
  tablecm <- round(t(t(table_cm) / colSums(as.matrix(table_cm))*100)) # crÃ©e les pourcentages
  tablemelt <- melt(tablecm)
  ggplot(tablemelt, aes(Reference, Prediction)) +
  geom_point(aes(size = value, color=value), alpha=0.8, show.legend=FALSE) +
  geom_text(aes(label = value), color="white") +
  scale_size(range = c(5,25)) +
  scale_y_discrete(limits = rev(levels(tablemelt$Prediction)))+
  theme_bw()
}

```

Dataset loading
```{r}
dta_ <- readRDS("..\\Data\\loan5.RDS")
```

Data sampling preparation
```{r}
sampling_factor=0.01

set.seed(17)
dta_sample_index <- createDataPartition(dta_$loan_status, p = sampling_factor, list = FALSE, times = 1)
paste("Nb Observation dans le sample : ", length(dta_sample_index))
dta_sample <- dta_[dta_sample_index,]
train.index <- createDataPartition(dta_sample$loan_status, p = .66, list = FALSE, times = 1)

dta_.train.x <- data.frame(dta_sample[train.index,!(colnames(dta_) %in% c("loan_status"))])
dta_.train.y <- as.factor(dta_sample[train.index,c("loan_status")])
dta_.test.x <- data.frame(dta_sample[-train.index,!(colnames(dta_) %in% c("loan_status"))])
dta_.test.y <- as.factor(dta_sample[-train.index,c("loan_status")])

#Level Name Compatibility
levels(dta_.train.y) <- c("CO","FP")
levels(dta_.test.y) <- c("CO","FP")

#Level ref = FP
dta_.train.y <- relevel(dta_.train.y, "CO")
dta_.test.y <- relevel(dta_.test.y, "CO")

#Proportion verification
table(dta_$loan_status)/sum(table(dta_$loan_status))
table(dta_.train.y)/sum(table(dta_.train.y))
table(dta_.test.y)/sum(table(dta_.test.y))
```


Parallel computing Setup
```{r}
detectCores() # nombre de coeurs sur la machine
cluster <- makeCluster(detectCores() - 1) # par convention on laisse un coeur pour l'OS
registerDoParallel(cluster)
```

Training Setup
```{r}
k_cv_sampling = 10
seed_list = lapply(0:k_cv_sampling,function(in_){seq(1:24)})
seed_list <- list.append(seed_list,17)
#seeds <- list(seed_list,seed_list,seed_list,17)
objControl <- trainControl(method="cv", number= k_cv_sampling, returnResamp='none', summaryFunction = prSummary, classProbs = TRUE, allowParallel = TRUE,seeds = seed_list)
#seeds = seeds has to be added to maintain the same seeds across trainings
```

Train Launch
```{r}
model_type = "parRF"
#getModelInfo(model_type)
gridsearch <- expand.grid(mtry=seq(1, 24, 1)) #
tune <- train(dta_.train.x,dta_.train.y,method = model_type, tuneGrid=gridsearch, trControl=objControl,metric='F')
tune
```

Closing Parallel Computing Instance
```{r}
stopCluster(cluster)
registerDoSEQ()
```

Results
```{r}
plot(tune)
paste('Best tune : ', tune$bestTune)
```

Predictions
```{r}
prediction <- predict(object=tune$finalModel, dta_.test.x, type='class')

head(prediction)
conf_matrix <- confusionMatrix(prediction, dta_.test.y)
conf_matrix
cm.plot(conf_matrix$table)
```




